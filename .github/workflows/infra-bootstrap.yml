name: Infrastructure Bootstrap

on:
  workflow_dispatch:
    inputs:
      destroy:
        description: "Type 'destroy' to tear down everything first"
        required: false
        default: ""

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: eu-west-2
  TF_ROLE: arn:aws:iam::630943284793:role/alex-counter-service-terraform-ci
  CLUSTER_NAME: alex-counter-service
  ARGOCD_NAMESPACE: argocd
  APP_NAMESPACE: prod

jobs:
  infra:
    name: "Terraform Infra"
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.8.0"

      - name: Terraform Init (infra)
        working-directory: terraform/infra
        run: terraform init

      - name: Terraform Destroy (infra)
        if: github.event.inputs.destroy == 'destroy'
        working-directory: terraform/infra
        run: terraform destroy -auto-approve
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_DB_PASSWORD }}

      - name: Import existing resources (idempotent)
        working-directory: terraform/infra
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_DB_PASSWORD }}
        run: |
          # Helper: skip import if the resource is already in state
          tf_import_if_missing() {
            local addr="$1"; local id="$2"
            terraform state show "$addr" &>/dev/null && echo "skip $addr (already in state)" || \
              terraform import "$addr" "$id" 2>/dev/null || true
          }

          tf_import_if_missing aws_ecr_repository.backend  counter-backend
          tf_import_if_missing aws_ecr_repository.frontend counter-frontend

          IGW_ID=$(aws ec2 describe-internet-gateways \
            --filters "Name=attachment.vpc-id,Values=$(aws eks describe-cluster \
              --name ${{ env.CLUSTER_NAME }} \
              --query 'cluster.resourcesVpcConfig.vpcId' --output text)" \
            --query "InternetGateways[0].InternetGatewayId" --output text 2>/dev/null)
          if [ -n "$IGW_ID" ] && [ "$IGW_ID" != "None" ]; then
            tf_import_if_missing aws_internet_gateway.this "$IGW_ID"
          fi

          OIDC_ISSUER=$(aws eks describe-cluster \
            --name ${{ env.CLUSTER_NAME }} \
            --query "cluster.identity.oidc.issuer" \
            --output text 2>/dev/null)
          if [ -n "$OIDC_ISSUER" ]; then
            OIDC_ARN=$(aws iam list-open-id-connect-providers \
              --query "OpenIDConnectProviderList[?ends_with(Arn,'${OIDC_ISSUER##*/}')].Arn" \
              --output text 2>/dev/null)
            if [ -n "$OIDC_ARN" ]; then
              tf_import_if_missing aws_iam_openid_connect_provider.eks "$OIDC_ARN"
            fi
          fi

          tf_import_if_missing aws_eks_addon.coredns                  "${{ env.CLUSTER_NAME }}:coredns"
          tf_import_if_missing aws_eks_addon.kube_proxy               "${{ env.CLUSTER_NAME }}:kube-proxy"
          tf_import_if_missing aws_eks_addon.vpc_cni                  "${{ env.CLUSTER_NAME }}:vpc-cni"
          tf_import_if_missing aws_eks_addon.metrics_server           "${{ env.CLUSTER_NAME }}:metrics-server"
          tf_import_if_missing aws_eks_addon.pod_identity_agent       "${{ env.CLUSTER_NAME }}:eks-pod-identity-agent"
          tf_import_if_missing aws_eks_addon.node_monitoring_agent    "${{ env.CLUSTER_NAME }}:eks-node-monitoring-agent"
          tf_import_if_missing aws_eks_addon.cloudwatch_observability "${{ env.CLUSTER_NAME }}:amazon-cloudwatch-observability"

          tf_import_if_missing aws_iam_role.node_group \
            "${{ env.CLUSTER_NAME }}-node-group-role"

          tf_import_if_missing aws_eks_node_group.default \
            "${{ env.CLUSTER_NAME }}:default"

          NODE_ROLE_ARN=$(aws iam get-role \
            --role-name "${{ env.CLUSTER_NAME }}-node-group-role" \
            --query "Role.Arn" --output text 2>/dev/null)
          if [ -n "$NODE_ROLE_ARN" ]; then
            tf_import_if_missing aws_eks_access_entry.node_group \
              "${{ env.CLUSTER_NAME }}:${NODE_ROLE_ARN}"
          fi

      - name: Terraform Apply (infra)
        working-directory: terraform/infra
        # -lock-timeout=5m: wait up to 5 min for a lock instead of failing immediately.
        # Prevents failures from brief lock contention between concurrent CI runs.
        run: terraform apply -auto-approve -lock-timeout=5m
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_DB_PASSWORD }}

  app-terraform:
    name: "Terraform App"
    runs-on: ubuntu-latest
    needs: infra

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.8.0"

      - name: Terraform Init (app)
        working-directory: terraform/app
        run: terraform init

      - name: Import existing IAM resources (idempotent)
        working-directory: terraform/app
        run: |
          # Helper: skip if already in state
          tf_import_if_missing() {
            local addr="$1"; local id="$2"
            terraform state show "$addr" &>/dev/null && echo "skip $addr (already in state)" || \
              terraform import "$addr" "$id" 2>/dev/null || true
          }

          GITHUB_OIDC_ARN=$(aws iam list-open-id-connect-providers \
            --query "OpenIDConnectProviderList[?ends_with(Arn,'token.actions.githubusercontent.com')].Arn" \
            --output text 2>/dev/null)
          if [ -n "$GITHUB_OIDC_ARN" ]; then
            tf_import_if_missing aws_iam_openid_connect_provider.github "$GITHUB_OIDC_ARN"
          fi

          tf_import_if_missing aws_iam_role.terraform_ci   alex-counter-service-terraform-ci
          tf_import_if_missing aws_iam_role.github_actions  alex-counter-service-github-actions-role
          tf_import_if_missing aws_iam_role.cluster_autoscaler alex-counter-service-cluster-autoscaler
          tf_import_if_missing aws_iam_role.adot_collector     alex-counter-service-adot-collector
          tf_import_if_missing aws_iam_role.alb_controller     alex-counter-service-alb-controller

          ALB_POLICY_ARN=$(aws iam list-policies \
            --query "Policies[?PolicyName=='alex-counter-service-alb-controller-policy'].Arn" \
            --output text 2>/dev/null)
          if [ -n "$ALB_POLICY_ARN" ]; then
            tf_import_if_missing aws_iam_policy.alb_controller "$ALB_POLICY_ARN"
          fi

          CF_ID=$(aws cloudfront list-distributions \
            --query "DistributionList.Items[?Comment=='alex-counter-service frontend'].Id" \
            --output text 2>/dev/null)
          if [ -n "$CF_ID" ]; then
            tf_import_if_missing aws_cloudfront_distribution.frontend "$CF_ID"
          fi

      - name: Terraform Apply (app)
        working-directory: terraform/app
        run: terraform apply -auto-approve -lock-timeout=5m

  argocd-install:
    name: "Install Argo CD"
    runs-on: ubuntu-latest
    needs: infra

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v4

      - name: Configure kubeconfig
        run: |
          aws eks update-kubeconfig \
            --region ${{ env.AWS_REGION }} \
            --name   ${{ env.CLUSTER_NAME }}

      - name: Create argocd namespace
        run: kubectl create namespace ${{ env.ARGOCD_NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Install / upgrade Argo CD
        run: |
          kubectl apply -n ${{ env.ARGOCD_NAMESPACE }} \
            --server-side --force-conflicts \
            -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml

      - name: Wait for Argo CD to be ready
        run: |
          kubectl rollout status deployment/argocd-server \
            -n ${{ env.ARGOCD_NAMESPACE }} \
            --timeout=300s

      - name: Ensure default AppProject exists
        run: |
          kubectl apply -f - <<'EOF'
          apiVersion: argoproj.io/v1alpha1
          kind: AppProject
          metadata:
            name: default
            namespace: argocd
          spec:
            sourceRepos:
              - "*"
            destinations:
              - namespace: "*"
                server: https://kubernetes.default.svc
            clusterResourceWhitelist:
              - group: "*"
                kind: "*"
          EOF

  # ── Stage 4: Observability stack (Prometheus + Grafana) ─────────────────────
  observability:
    name: "Observability Stack"
    runs-on: ubuntu-latest
    needs: infra

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v4

      - name: Install Helm
        uses: azure/setup-helm@v4

      - name: Configure kubeconfig
        run: |
          aws eks update-kubeconfig \
            --region ${{ env.AWS_REGION }} \
            --name   ${{ env.CLUSTER_NAME }}

      - name: Add Prometheus community Helm repo
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo update

      - name: Seed Grafana admin secret
        run: |
          kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
          kubectl create secret generic grafana-admin \
            --namespace=monitoring \
            --from-literal=admin-user=admin \
            --from-literal=admin-password=${{ secrets.GRAFANA_ADMIN_PASSWORD }} \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Install / upgrade kube-prometheus-stack
        run: |
          helm upgrade --install kube-prometheus-stack \
            prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --values manifests/observability/kube-prometheus-values.yaml \
            --wait --timeout 5m

      - name: Print Grafana access info
        run: |
          echo "## Grafana (metrics)" >> $GITHUB_STEP_SUMMARY
          echo "Port-forward: \`kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 3000:80\`" >> $GITHUB_STEP_SUMMARY
          echo "URL: http://localhost:3000 — admin / (GRAFANA_ADMIN_PASSWORD secret)" >> $GITHUB_STEP_SUMMARY

  # ── Stage 5: Seed Kubernetes secrets + deploy via Argo CD ───────────────────
  deploy:
    name: "Deploy App"
    runs-on: ubuntu-latest
    needs: [app-terraform, argocd-install, observability]

    steps:
      - uses: actions/checkout@v4

      - uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install kubectl
        uses: azure/setup-kubectl@v4

      - name: Configure kubeconfig
        run: |
          aws eks update-kubeconfig \
            --region ${{ env.AWS_REGION }} \
            --name   ${{ env.CLUSTER_NAME }}

      - name: Read Terraform outputs
        working-directory: terraform/infra
        run: |
          terraform init -input=false
          echo "RDS_ENDPOINT=$(terraform output -raw rds_endpoint)"     >> $GITHUB_ENV
          echo "BACKEND_ECR=$(terraform output -raw backend_ecr_url)"   >> $GITHUB_ENV
          echo "FRONTEND_ECR=$(terraform output -raw frontend_ecr_url)" >> $GITHUB_ENV
        env:
          TF_VAR_db_password: ${{ secrets.TF_VAR_DB_PASSWORD }}

      - name: Create prod namespace
        run: kubectl create namespace ${{ env.APP_NAMESPACE }} --dry-run=client -o yaml | kubectl apply -f -

      - name: Create / update backend-db secret
        run: |
          kubectl create secret generic backend-db \
            --namespace=${{ env.APP_NAMESPACE }} \
            --from-literal=DB_PASSWORD=${{ secrets.TF_VAR_DB_PASSWORD }} \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Create / update backend-config ConfigMap
        run: |
          kubectl create configmap backend-config \
            --namespace=${{ env.APP_NAMESPACE }} \
            --from-literal=DB_HOST=${{ env.RDS_ENDPOINT }} \
            --from-literal=DB_PORT=5432 \
            --from-literal=DB_NAME=counter \
            --from-literal=DB_USER=counter \
            --from-literal=OTEL_SERVICE_NAME=counter-backend \
            --from-literal=OTEL_ENV=prod \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: Apply Argo CD Application
        run: kubectl apply -f manifests/argocd/application.yaml

      - name: Trigger Argo CD sync
        run: |
          ARGOCD_PASSWORD=$(kubectl -n ${{ env.ARGOCD_NAMESPACE }} get secret argocd-initial-admin-secret \
            -o jsonpath="{.data.password}" | base64 -d)

          curl -sSL -o /usr/local/bin/argocd \
            https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
          chmod +x /usr/local/bin/argocd

          kubectl port-forward svc/argocd-server -n ${{ env.ARGOCD_NAMESPACE }} 8080:443 &
          sleep 5

          argocd login localhost:8080 \
            --username admin \
            --password "$ARGOCD_PASSWORD" \
            --insecure

          argocd app sync counter-service --force || true

      - name: Summary
        run: |
          echo "## Bootstrap complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Resource | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|---|---|" >> $GITHUB_STEP_SUMMARY
          echo "| Cluster | ${{ env.CLUSTER_NAME }} |" >> $GITHUB_STEP_SUMMARY
          echo "| RDS | ${{ env.RDS_ENDPOINT }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Backend ECR | ${{ env.BACKEND_ECR }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Frontend ECR | ${{ env.FRONTEND_ECR }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Access" >> $GITHUB_STEP_SUMMARY
          echo "| Tool | Command |" >> $GITHUB_STEP_SUMMARY
          echo "|---|---|" >> $GITHUB_STEP_SUMMARY
          echo "| Argo CD | \`kubectl port-forward svc/argocd-server -n argocd 8080:443\` → https://localhost:8080 |" >> $GITHUB_STEP_SUMMARY
          echo "| Grafana (metrics) | \`kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 3000:80\` → http://localhost:3000 (admin/admin) |" >> $GITHUB_STEP_SUMMARY
          echo "| Prometheus | \`kubectl port-forward svc/kube-prometheus-stack-prometheus -n monitoring 9090:9090\` → http://localhost:9090 |" >> $GITHUB_STEP_SUMMARY
          echo "| Logs | AWS Console → CloudWatch → Log groups → /aws/containerinsights/${{ env.CLUSTER_NAME }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Traces | AWS Console → X-Ray → Traces |" >> $GITHUB_STEP_SUMMARY
